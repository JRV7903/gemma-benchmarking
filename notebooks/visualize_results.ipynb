{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a12ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualization script for Gemma benchmarking results.\n",
    "\n",
    "This script can be run directly or converted to a Jupyter notebook using:\n",
    "jupyter nbconvert --to notebook --execute visualize_results.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9168c26c",
   "metadata": {},
   "source": [
    "# Gemma Benchmarking Results\n",
    "\n",
    "This notebook visualizes the results from the Gemma benchmarking framework, comparing Gemma 2B, Gemma 7B, and Mistral 7B models on the MMLU dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d24dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = project_root / \"results\"\n",
    "\n",
    "result_dirs = sorted([d for d in results_dir.glob(\"*\") if d.is_dir()], \n",
    "                     key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "\n",
    "if not result_dirs:\n",
    "    print(\"No results found. Please run the benchmark first.\")\n",
    "else:\n",
    "    latest_results = result_dirs[0]\n",
    "    print(f\"Loading results from: {latest_results}\")\n",
    "    \n",
    "    metrics_path = latest_results / \"metrics.csv\"\n",
    "    if metrics_path.exists():\n",
    "        metrics_df = pd.read_csv(metrics_path)\n",
    "        print(f\"Loaded metrics data with {len(metrics_df)} rows\")\n",
    "    else:\n",
    "        print(f\"Metrics file not found: {metrics_path}\")\n",
    "        metrics_df = None\n",
    "    \n",
    "    predictions_path = latest_results / \"predictions.csv\"\n",
    "    if predictions_path.exists():\n",
    "        predictions_df = pd.read_csv(predictions_path)\n",
    "        print(f\"Loaded predictions data with {len(predictions_df)} rows\")\n",
    "    else:\n",
    "        print(f\"Predictions file not found: {predictions_path}\")\n",
    "        predictions_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a5f0ad",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Let's visualize the performance of different models across different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'metrics_df' in locals() and metrics_df is not None:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    overall = metrics_df[metrics_df['category'] == 'overall']\n",
    "    \n",
    "    sns.barplot(x='model', y='accuracy', data=overall)\n",
    "    plt.title('Overall Accuracy by Model')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d505202",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'metrics_df' in locals() and metrics_df is not None:\n",
    "    categories = metrics_df[metrics_df['category'] != 'overall']\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(x='category', y='accuracy', hue='model', data=categories)\n",
    "    plt.title('Accuracy by Category and Model')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf509803",
   "metadata": {},
   "source": [
    "## Model Size vs. Performance\n",
    "\n",
    "Let's analyze the relationship between model size and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e829b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'metrics_df' in locals() and metrics_df is not None and 'overall' in locals():\n",
    "    model_sizes = {\n",
    "        'gemma-2b': 2,\n",
    "        'gemma-7b': 7,\n",
    "        'mistral-7b': 7\n",
    "    }\n",
    "    \n",
    "    overall['model_size'] = overall['model'].map(model_sizes)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for model in overall['model'].unique():\n",
    "        model_data = overall[overall['model'] == model]\n",
    "        plt.scatter(model_data['model_size'], model_data['accuracy'], \n",
    "                   label=model, s=100)\n",
    "    \n",
    "    plt.title('Model Size vs. Accuracy')\n",
    "    plt.xlabel('Model Size (Billions of Parameters)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6791e247",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis compares the performance of Gemma 2B, Gemma 7B, and Mistral 7B models on the MMLU benchmark. The visualizations show how these models perform across different categories, highlighting their relative strengths and weaknesses. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
